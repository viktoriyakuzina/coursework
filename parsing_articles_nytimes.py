# -*- coding: utf-8 -*-
"""parsing_articles_nytimes (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OQSSVbaF2uWcIOEyBu9xTB-BKQ4S-xnv
"""

import requests
import json
import pandas as pd
import time

data = pd.DataFrame() 
users = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36',
        'My User Agent 1.0']

iteration = 0
page = 1
year = 2020 
# так как NYT ставит ограничение на количество запросов (каждые 2000 статей NYT закрывал доступ), 
# то в момент, когда ошибка будет без конца выдаваться, нужно остановить выполнение кода, 
# посмотреть, на какой дате остановилась загрузка и сменить даты в переменных year, begin_date, end_date
# страна меняется в query, как выгрузятся все метаданные
condition = True
while condition:
    print(year, page)
    params = {'q': 'Russia',
              'sort': 'oldest',
             'begin_date': '{}1224'.format(year), 
             'end_date': '{}0101'.format(year+1),
             'api-key': 'DoOw3RcEIlPSttmDUhLTpDbwl7xu916l',
             'page': '{}'.format(page)}
    url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?'
        
    try: 
        html = requests.get(url, params, headers = {'User-agent': users[iteration % 3]})
        print(html)
        
        if len(json.loads(html.text)['response']['docs']) == 0:
            print('NEXT YEAR')
            year += 1
            page = 0
            continue
        
        if data.empty:
            data = pd.DataFrame(json.loads(html.text)['response']['docs'])
        else:
            data = pd.concat([data, pd.DataFrame(json.loads(html.text)['response']['docs'])])
        page += 1
        
    except:
        print('Error')
        time.sleep(30)
        continue
        
    if data['pub_date'].apply(lambda x: int(x[:4])).value_counts()[year] > 4500:
        print('NEXT YEAR')
        year += 1
        page = 1
        
    if year == 2021:
        condition = False
    
    iteration += 1

data['headline'] = data['headline'].apply(lambda x: x['main']) # переименование колонки

data = data.reset_index(drop = True) # сброс индексов

data['pub_date'] = pd.to_datetime(data['pub_date']) # переход к нужному типу данных (к дате)
data['pub_date'].dt.year.value_counts() # проверка, сколько наблюдений для каждого года выгрузилось

data['pub_date'] # проверка, на каких датах остановилась выгрузка статей

data.to_csv('urls_RU_2020_12_24.csv') # сохранение датасета в формате csv